{
    "contents" : "---\ntitle: \"Practical Machine Learning\"\noutput: html_document\n---\n\nIn this project I am going to analice data obtained from deviceses such as Jawbone Up, Nike FuelBand, and Fitbit and predict th manner in which the subjects did the exercise.\n\n```{r}\nlibrary(caret)\nlibrary(rpart) \nlibrary(rpart.plot)\nlibrary(RColorBrewer)\nlibrary(rattle)\nlibrary(randomForest)\nset.seed(111)\n```\n\n###Data \n\nThe training data for this project are available here: \n\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\n\nThe test data are available here: \n\nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\n\nThe data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment. \n\n\n```{r}\ntrainUrl <- \"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\ntestUrl <- \"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\ntraining <- read.csv(url(trainUrl), na.strings=c(\"NA\",\"#DIV/0!\",\"\"))\ntesting <- read.csv(url(testUrl), na.strings=c(\"NA\",\"#DIV/0!\",\"\"))\n```\n\n###Cross-validation procedure\n\nI have decide to split training data in two (60% to build the model and 40% to test it)\n\n```{r}\ninTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)\nmyTraining <- training[inTrain, ]\nmyTesting <- training[-inTrain, ]\n```\n\n###Cleaning data\n\nNow I am going to remove all variables which are near zero variance, so they don't make nothing to the model.\n\n```{r}\nnearZeroVars <- nearZeroVar(myTraining, saveMetrics=TRUE)\nZeroVars <- names(myTraining) %in% c(\"new_window\", \"kurtosis_roll_belt\", \"kurtosis_picth_belt\",\n\"kurtosis_yaw_belt\", \"skewness_roll_belt\", \"skewness_roll_belt.1\", \"skewness_yaw_belt\",\n\"max_yaw_belt\", \"min_yaw_belt\", \"amplitude_yaw_belt\", \"avg_roll_arm\", \"stddev_roll_arm\",\n\"var_roll_arm\", \"avg_pitch_arm\", \"stddev_pitch_arm\", \"var_pitch_arm\", \"avg_yaw_arm\",\n\"stddev_yaw_arm\", \"var_yaw_arm\", \"kurtosis_roll_arm\", \"kurtosis_picth_arm\",\n\"kurtosis_yaw_arm\", \"skewness_roll_arm\", \"skewness_pitch_arm\", \"skewness_yaw_arm\",\n\"max_roll_arm\", \"min_roll_arm\", \"min_pitch_arm\", \"amplitude_roll_arm\", \"amplitude_pitch_arm\",\n\"kurtosis_roll_dumbbell\", \"kurtosis_picth_dumbbell\", \"kurtosis_yaw_dumbbell\", \"skewness_roll_dumbbell\",\n\"skewness_pitch_dumbbell\", \"skewness_yaw_dumbbell\", \"max_yaw_dumbbell\", \"min_yaw_dumbbell\",\n\"amplitude_yaw_dumbbell\", \"kurtosis_roll_forearm\", \"kurtosis_picth_forearm\", \"kurtosis_yaw_forearm\",\n\"skewness_roll_forearm\", \"skewness_pitch_forearm\", \"skewness_yaw_forearm\", \"max_roll_forearm\",\n\"max_yaw_forearm\", \"min_roll_forearm\", \"min_yaw_forearm\", \"amplitude_roll_forearm\",\n\"amplitude_yaw_forearm\", \"avg_roll_forearm\", \"stddev_roll_forearm\", \"var_roll_forearm\",\n\"avg_pitch_forearm\", \"stddev_pitch_forearm\", \"var_pitch_forearm\", \"avg_yaw_forearm\",\n\"stddev_yaw_forearm\", \"var_yaw_forearm\")\nmyTraining <- myTraining[!ZeroVars]\n```\n\nRemove the colum Dataset-ID\n```{r}\nmyTraining <- myTraining[c(-1)]\n```\n\nRemove variables with many NAs (more than 60%)\n```{r}\nmyCleanTraining <- myTraining\nfor(i in 1:length(myTraining)) { \n        if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .6 ) { #if n?? NAs > 60% of total observations\n        for(j in 1:length(myCleanTraining)) {\n            if( length( grep(names(myTraining[i]), names(myCleanTraining)[j]) ) ==1)  { \n                myCleanTraining <- myCleanTraining[ , -j] \n            }   \n        } \n    }\n}\nmyTraining <- myCleanTraining\nrm(myCleanTraining)\n```\n\nNow I am removing the same columns in the tests dataset.\n\n```{r}\nclean1 <- colnames(myTraining)\nclean2 <- colnames(myTraining[, -58]) #classe column\nmyTesting <- myTesting[clean1]\ntesting <- testing[clean2]\n```\n\nIn order to ensure proper functioning of Decision Trees and especially RandomForest Algorithm with the Test data set (data set provided), we need to coerce the data into the same type.\n\n```{r}\nfor (i in 1:length(testing) ) {\n        for(j in 1:length(myTraining)) {\n        if( length( grep(names(myTraining[i]), names(testing)[j]) ) ==1)  {\n            class(testing[j]) <- class(myTraining[i])\n        }      \n    }      \n}\n#And to make sure Coertion really worked, simple smart ass technique:\ntesting <- rbind(myTraining[2, -58] , testing) #note row 2 does not mean anything, this will be removed right.. now:\ntesting <- testing[-1,]\n```\n\n##Decision tree\n\n```{r}\nmodFit1 <- rpart(classe ~ ., data=myTraining, method=\"class\")\nfancyRpartPlot(modFit1)\n```\n\nChecking the results.\n```{r}\npredictions1 <- predict(modFit1, myTesting, type = \"class\")\nconfusionMatrix(predictions1, myTesting$classe)\n```\n\n##Random Forest\n```{r}\nmodFit2 <- randomForest(classe ~. , data=myTraining)\npredictions2 <- predict(modFit2, myTesting, type = \"class\")\nconfusionMatrix(predictions2, myTesting$classe)\n\n```\n\nRandom forest show better results.\n\n##Test our model\n\nWe'll use random forest.\n```{r}\npredictionsFinal <- predict(modFit2, testing, type = \"class\")\n\npml_write_files = function(x){\n  n = length(x)\n  for(i in 1:n){\n    filename = paste0(\"problem_id_\",i,\".txt\")\n    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\n  }\n}\n\npml_write_files(predictionsFinal)\n\n```",
    "created" : 1440348776625.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3056760156",
    "id" : "224C1DE2",
    "lastKnownWriteTime" : 1440348841,
    "path" : "C:/Users/Daniel/Dropbox/Cursos/Data Science/Practical Machine Learning/project/practical machine learning.Rmd",
    "project_path" : "practical machine learning.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_markdown"
}